<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Mist","darkmode":true,"version":"8.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>
<meta name="description" content="机器学习中的算法涉及诸多的优化问题，典型的就是利用梯度下降法(gradient descent)求使损失函数\(J(\theta)\)下降的模型参数 \(\theta\)。">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习算法优化">
<meta property="og:url" content="http://example.com/2021/11/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%BC%98%E5%8C%96/index.html">
<meta property="og:site_name" content="whlll-blog">
<meta property="og:description" content="机器学习中的算法涉及诸多的优化问题，典型的就是利用梯度下降法(gradient descent)求使损失函数\(J(\theta)\)下降的模型参数 \(\theta\)。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/whlll-coder/images/202111011808791.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/whlll-coder/images/202111011835413.png">
<meta property="article:published_time" content="2021-11-01T07:35:09.000Z">
<meta property="article:modified_time" content="2021-11-01T12:59:53.777Z">
<meta property="article:author" content="whlll">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="算法">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/whlll-coder/images/202111011808791.png">


<link rel="canonical" href="http://example.com/2021/11/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%BC%98%E5%8C%96/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://example.com/2021/11/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%BC%98%E5%8C%96/","path":"2021/11/01/深度学习算法优化/","title":"深度学习算法优化"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>深度学习算法优化 | whlll-blog</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">whlll-blog</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#bath-gradlient-descen-bgd"><span class="nav-text">Bath Gradlient Descen (BGD)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%9B%B4%E6%96%B0%E8%A7%84%E5%88%99"><span class="nav-text">梯度更新规则</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BC%BA%E7%82%B9"><span class="nav-text">缺点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81"><span class="nav-text">代码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%82%E7%94%A8%E8%8C%83%E5%9B%B4"><span class="nav-text">适用范围</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#stochastic-gradient-descent-sgd"><span class="nav-text">Stochastic Gradient Descent (SGD)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%9B%B4%E6%96%B0%E8%A7%84%E5%88%99-1"><span class="nav-text">梯度更新规则</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BC%BA%E7%82%B9-1"><span class="nav-text">缺点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81-1"><span class="nav-text">代码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%82%E7%94%A8%E8%8C%83%E5%9B%B4-1"><span class="nav-text">适用范围</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mini-batch-gradient-descent-mbgd"><span class="nav-text">Mini-Batch Gradient Descent (MBGD)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%9B%B4%E6%96%B0%E8%A7%84%E5%88%99-2"><span class="nav-text">梯度更新规则</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BC%BA%E7%82%B9-2"><span class="nav-text">缺点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81-2"><span class="nav-text">代码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%82%E7%94%A8%E8%8C%83%E5%9B%B4-2"><span class="nav-text">适用范围</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#momentun"><span class="nav-text">Momentun</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%9B%B4%E6%96%B0%E8%A7%84%E5%88%99-3"><span class="nav-text">梯度更新规则</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BC%BA%E7%82%B9-3"><span class="nav-text">缺点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81-3"><span class="nav-text">代码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%82%E7%94%A8%E8%8C%83%E5%9B%B4-3"><span class="nav-text">适用范围</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#nesterov-accelerated-gradient"><span class="nav-text">Nesterov Accelerated Gradient</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%9B%B4%E6%96%B0%E8%A7%84%E5%88%99-4"><span class="nav-text">梯度更新规则</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%82%E7%94%A8%E8%8C%83%E5%9B%B4-4"><span class="nav-text">适用范围</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81-4"><span class="nav-text">代码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%82%E7%94%A8%E8%8C%83%E5%9B%B4-5"><span class="nav-text">适用范围</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#adagradadaptive-gradient-algorithm"><span class="nav-text">Adagrad(Adaptive gradient algorithm)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%9B%B4%E6%96%B0%E8%A7%84%E5%88%99-5"><span class="nav-text">梯度更新规则</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BC%BA%E7%82%B9-4"><span class="nav-text">缺点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81-5"><span class="nav-text">代码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E8%8C%83%E5%9B%B4"><span class="nav-text">使用范围</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#adadelta-rmsprop"><span class="nav-text">Adadelta (RMSProp)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%9B%B4%E6%96%B0%E8%A7%84%E5%88%99-6"><span class="nav-text">梯度更新规则</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BC%BA%E7%82%B9-5"><span class="nav-text">缺点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81-6"><span class="nav-text">代码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%82%E7%94%A8%E8%8C%83%E5%9B%B4-6"><span class="nav-text">适用范围</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#adam"><span class="nav-text">Adam</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%9B%B4%E6%96%B0%E8%A7%84%E5%88%99-7"><span class="nav-text">梯度更新规则</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BC%BA%E7%82%B9-6"><span class="nav-text">缺点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81-7"><span class="nav-text">代码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%82%E7%94%A8%E8%8C%83%E5%9B%B4-7"><span class="nav-text">适用范围</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="whlll"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">whlll</p>
  <div class="site-description" itemprop="description">whlll的小破站❥</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">22</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">28</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="sidebar-button site-overview-item animated">
    <button><i class="fa fa-comment"></i>
      Chat with me
    </button>
  </div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/whlll-coder" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;whlll-coder" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:whlll0305lhl@gmail.com" title="E-Mail → mailto:whlll0305lhl@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/u/7193773610" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;u&#x2F;7193773610" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a>
      </span>
  </div>


  <div class="links-of-blogroll site-overview-item animated">
    <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.jmyblog.top/" title="https:&#x2F;&#x2F;www.jmyblog.top&#x2F;" rel="noopener" target="_blank">谁把钱丢了</a>
        </li>
    </ul>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

      
            <script type="text/javascript" charset="utf-8" src="/js/tagcloud.js"></script>
            <script type="text/javascript" charset="utf-8" src="/js/tagcanvas.js"></script>
            <div class="widget-wrap">
                <h3 class="widget-title">都有啥</h3>
                <div id="myCanvasContainer" class="widget tagcloud">
                    <canvas width="250" height="250" id="resCanvas" style="width=100%">
                        <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/408%E8%80%83%E7%A0%94/" rel="tag">408考研</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Docker/" rel="tag">Docker</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hibernate/" rel="tag">Hibernate</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/JVM/" rel="tag">JVM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Jasmin/" rel="tag">Jasmin</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Jvm/" rel="tag">Jvm</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Markdown/" rel="tag">Markdown</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MindStudio/" rel="tag">MindStudio</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MySql/" rel="tag">MySql</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Mybatis/" rel="tag">Mybatis</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Netty/" rel="tag">Netty</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pytorch/" rel="tag">Pytorch</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RabbitMq/" rel="tag">RabbitMq</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SpringBoot/" rel="tag">SpringBoot</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/vue/" rel="tag">vue</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/zookeeper/" rel="tag">zookeeper</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/" rel="tag">中间件</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BA%BA%E8%84%B8%E6%B3%A8%E5%86%8C/" rel="tag">人脸注册</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90%E5%99%A8/" rel="tag">代码生成器</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8D%8E%E4%B8%BA/" rel="tag">华为</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BB%BA%E7%AB%99%E6%8C%87%E5%8D%97/" rel="tag">建站指南</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/" rel="tag">数据库</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%9F%E6%9C%AB%E7%A0%B4%E9%98%B2/" rel="tag">期末破防</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/" rel="tag">消息中间件</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%AE%97%E6%B3%95/" rel="tag">算法</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/" rel="tag">计算机网络</a><span class="tag-list-count">1</span></li></ul>
                    </canvas>
                </div>
            </div>
      

    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/whlll-coder" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/11/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%BC%98%E5%8C%96/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="whlll">
      <meta itemprop="description" content="whlll的小破站❥">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="whlll-blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          深度学习算法优化
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2021-11-01 15:35:09 / 修改时间：20:59:53" itemprop="dateCreated datePublished" datetime="2021-11-01T15:35:09+08:00">2021-11-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%B7%A5%E4%BD%9C%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">工作学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>5.1k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>5 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>机器学习中的算法涉及诸多的优化问题，典型的就是利用梯度下降法(gradient descent)求使损失函数<span class="math inline">\(J(\theta)\)</span>下降的模型参数 <span class="math inline">\(\theta\)</span>。</p>
<span id="more"></span>
<h2 id="bath-gradlient-descen-bgd">Bath Gradlient Descen (BGD)</h2>
<blockquote>
<p>采用整个训练集的数据来计算 cost function 对参数的梯度</p>
</blockquote>
<h3 id="梯度更新规则">梯度更新规则</h3>
<p><span class="math display">\[
\theta = \theta-\eta\nabla_\theta(\theta)
\]</span></p>
<h3 id="缺点">缺点</h3>
<p>计算慢，遇到大量数据集棘手，不能投入新数据实时更新模型</p>
<h3 id="代码">代码</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nb_epochs):</span><br><span class="line">  params_grad = evaluate_gradient(loss_function, data, params)</span><br><span class="line">  params = params - learning_rate * params_grad</span><br></pre></td></tr></table></figure>
<blockquote>
<p>nb_epochs -&gt; 事先定义的迭代次数</p>
<p>params_grad -&gt; 梯度向量</p>
<p>沿梯度方向更新参数 params</p>
<p>learning_rate -&gt; 学习速率</p>
</blockquote>
<h3 id="适用范围">适用范围</h3>
<p><font color=red><strong>Batch gradient descent 对于凸函数可以收敛到全局极小值，对于非凸函数可以收敛到局部极小值。</strong></font></p>
<h2 id="stochastic-gradient-descent-sgd">Stochastic Gradient Descent (SGD)</h2>
<blockquote>
<p>SGD 每次更新只对<strong>每个</strong>样本进行梯度更新</p>
</blockquote>
<p><font color=red>一次只进行一次更新 没有冗余 可以新增样本</font></p>
<h3 id="梯度更新规则-1">梯度更新规则</h3>
<p><span class="math display">\[
\theta=\theta-\eta\nabla_\theta{J}(\theta;x^i;y^i)
\]</span></p>
<h3 id="缺点-1">缺点</h3>
<p>噪音比BGD多，使得SGD并不是每次迭代都是向着整体最优方向，所以虽然训练速度快，但是准确度下降，并不是全局最优(cost function 震荡)</p>
<blockquote>
<p>稍微减小 learning rate SGD 和 BGD的收敛性是一样的</p>
</blockquote>
<h3 id="代码-1">代码</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nb_epochs):</span><br><span class="line">  np.random.shuffle(data)</span><br><span class="line">  <span class="keyword">for</span> example <span class="keyword">in</span> data:</span><br><span class="line">    params_grad = evaluate_gradient(loss_function, example, params)</span><br><span class="line">    params = params - learning_rate * params_grad</span><br></pre></td></tr></table></figure>
<blockquote>
<p>整体数据集加了一个对样本的循环</p>
</blockquote>
<h3 id="适用范围-1">适用范围</h3>
<p><font color=red><strong>BGD 可以收敛到局部极小值，当然 SGD 的震荡可能会跳到更好的局部极小值处。</strong></font></p>
<h2 id="mini-batch-gradient-descent-mbgd">Mini-Batch Gradient Descent (MBGD)</h2>
<blockquote>
<p>MBGD 每一次利用一小批样本，即<strong>n个</strong>样本进行计算</p>
</blockquote>
<ul>
<li><p><font color=red><strong>可以降低参数更新时的方差，收敛更稳定</strong></font></p></li>
<li><p><font color=red><strong>方便用库中高度优化的矩阵操作来进行梯度的计算</strong></font></p></li>
</ul>
<h3 id="梯度更新规则-2">梯度更新规则</h3>
<p><span class="math display">\[
\theta=\theta-\eta\nabla{J}(\theta;x^{i:i+n};y^{i:i+n})
\]</span></p>
<h3 id="缺点-2">缺点</h3>
<ul>
<li><strong>会在鞍点或者局部最小点震荡跳动，每次找到的梯度都是不同的，就会发生震荡，来回跳动。</strong></li>
<li>需要挑选一个合适的学习率</li>
</ul>
<h3 id="代码-2">代码</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># n 一般取值在 50~256</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nb_epochs):</span><br><span class="line">  np.random.shuffle(data)</span><br><span class="line">  <span class="keyword">for</span> batch <span class="keyword">in</span> get_batches(data, batch_size=<span class="number">50</span>):</span><br><span class="line">    params_grad = evaluate_gradient(loss_function, batch, params)</span><br><span class="line">    params = params - learning_rate * params_grad</span><br></pre></td></tr></table></figure>
<h3 id="适用范围-2">适用范围</h3>
<p>同SGD</p>
<h2 id="momentun">Momentun</h2>
<blockquote>
<p>动量算法积累了之前梯度指数级衰减的移动平均，并且继续沿该方向移动。从形式上看，动量算法引入了变量 <span class="math inline">\(v\)</span> 充当速度角色-代表参数在参数空间移动的方向和速率，速度被认为是负梯度的指数衰减平均</p>
</blockquote>
<p><img src="https://cdn.jsdelivr.net/gh/whlll-coder/images/202111011808791.png" /></p>
<p><font color=red><strong>通过加入变量v 使梯度继续沿原方向移动</strong></font></p>
<h3 id="梯度更新规则-3">梯度更新规则</h3>
<p><span class="math display">\[
v_t=\gamma_{t-1}+\eta\nabla_\theta{J}(\theta)
\]</span></p>
<p><span class="math display">\[
\theta=\theta-v_t
\]</span></p>
<p><font color=red>一般取 <span class="math inline">\(\gamma=0.9\)</span></font></p>
<h3 id="缺点-3">缺点</h3>
<p>需要人工设定学习率</p>
<h3 id="代码-3">代码</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vx = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    dx = computed_gradient(x)</span><br><span class="line">    vx = rho * vx + dx</span><br><span class="line">    x += - learning_rate * vx</span><br></pre></td></tr></table></figure>
<h3 id="适用范围-3">适用范围</h3>
<blockquote>
<p>SGD只依赖于当前迭代的梯度，十分不稳定，加一个“动量”的话，相当于有了一个惯性在里面，梯度方向不仅与这次的迭代有关，还与之前一次的迭代结果有关。“当前一次效果好的话，就加快步伐；当前一次效果不好的话，就减慢步伐”；而且在局部最优值处，没有梯度但因为还存在一个动量，可以跳出局部最优值</p>
</blockquote>
<h2 id="nesterov-accelerated-gradient">Nesterov Accelerated Gradient</h2>
<blockquote>
<p>Nesterov动量和标准动量之间的区别在于梯度的计算上。Nesterov动量中，梯度计算在施加当前速度之后，可以理解为Nesterov 动量往标准动量方法中添加了一个校正因子</p>
</blockquote>
<h3 id="梯度更新规则-4">梯度更新规则</h3>
<p><span class="math display">\[
v_t = \gamma v_{t-1}+\eta \nabla_\theta J(\theta-\gamma v_t)
\]</span></p>
<p><span class="math display">\[
\theta = \theta-v_t
\]</span></p>
<p><font color=red>一般取 <span class="math inline">\(\gamma=0.9\)</span></font></p>
<h3 id="适用范围-4">适用范围</h3>
<p><img src="https://cdn.jsdelivr.net/gh/whlll-coder/images/202111011835413.png" /></p>
<blockquote>
<p>蓝色是 Momentum 的过程，会先计算当前的梯度，然后在更新后的累积梯度后会有一个大的跳跃 而 NAG 会先在前一步的累积梯度上(brown vector)有一个大的跳跃，然后衡量一下梯度做一下修正(red vector)，这种预期的更新可以避免我们走的太快</p>
</blockquote>
<h3 id="代码-4">代码</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">	dx = compute_gradient(x)</span><br><span class="line">	old_v = v</span><br><span class="line">	v = rho * v - learning_rate * dx</span><br><span class="line">	x += - rho * old_v + (<span class="number">1</span>+rho) * v</span><br></pre></td></tr></table></figure>
<h3 id="适用范围-5">适用范围</h3>
<p>目前为止，我们可以做到，<strong>在更新梯度时顺应 loss function 的梯度来调整速度，并且对 SGD 进行加速</strong>。</p>
<h2 id="adagradadaptive-gradient-algorithm">Adagrad(Adaptive gradient algorithm)</h2>
<blockquote>
<p>独立地适应所有模型参数的学习率，缩放每个参数反比于其所有梯度历史平方值总和和平方根，具有损失最大偏导的参数相应有一个快速下降的学习率，而具有小偏导的参数在学习率上有相对较小的下降。总的效果是在参数空间中更为平缓的倾斜方向会取得更大的进步</p>
</blockquote>
<h3 id="梯度更新规则-5">梯度更新规则</h3>
<p><span class="math display">\[
\theta_{t+1} = \theta_{t,i} - \frac \eta {\sqrt{G_{t,ii}+\epsilon}}g_{t,i}
\]</span></p>
<p>其中g为 t时刻 参数 <span class="math inline">\(\theta_i\)</span> 的梯度 <span class="math display">\[
g_{t,i}=\nabla_\theta J(\theta_i)
\]</span> 其中<span class="math inline">\(G_t\)</span>是个对角矩阵，(i,i)元素就是t时刻参数<span class="math inline">\(\theta _i\)</span>的梯度平方和</p>
<p>一般 <span class="math inline">\(\eta\)</span> 选取 0.01</p>
<h3 id="缺点-4">缺点</h3>
<p>分母会不断积累，这样学习率就会收缩并最终变得非常小</p>
<h3 id="代码-5">代码</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">grad_squared = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    dx = compute_gradient(x)</span><br><span class="line">    grad_squared += dx * dx</span><br><span class="line">    x -= learning_rate * dx / (np.sqrt(grad_squared) + <span class="number">1e-7</span>)</span><br></pre></td></tr></table></figure>
<h3 id="使用范围">使用范围</h3>
<blockquote>
<p>总的效果是在参数空间中更为平缓的倾斜方向会取得更大的进步。对于训练深度神经网络而言，从训练开始积累梯度平方会导致有效学习率过早和过量的减小</p>
</blockquote>
<h2 id="adadelta-rmsprop">Adadelta (RMSProp)</h2>
<blockquote>
<p>对 Adagrad 的改进</p>
</blockquote>
<p>和 Adagrad 相比，分母的G换成了过去的梯度平方的衰减平均值 <font color=red>指数衰减平均值</font> <span class="math display">\[
\Delta\theta_t=-\frac \eta {E[g^2]_t+\epsilon}g_t
\]</span> 这个分母相当于<strong>梯度的均方根 root mean squared (RMS)</strong>，在数据统计分析中，将所有值平方求和，求其均值，再开平方，就得到均方根值 ，所以可以用 RMS 简写 <span class="math display">\[
\Delta\theta_t=-\frac \eta {RMS[g]_t}g_t
\]</span> 其中 E 的计算公式如下，t 时刻的依赖于前一时刻的平均和当前的梯度 <span class="math display">\[
E[g^2]_t=\gamma E[g^2]_{t-1}+(1-\gamma){g_t}^2
\]</span></p>
<h3 id="梯度更新规则-6">梯度更新规则</h3>
<p>将学习率 <span class="math inline">\(\eta\)</span> 换成 <span class="math inline">\(RMS[\Delta \theta]\)</span> <span class="math display">\[
\Delta\theta_t=-\frac {RMS[\Delta \theta]_{t-1}} {RMS[g]_t}g_t
\]</span></p>
<p><span class="math display">\[
\theta_{t+1}=\theta_t+\Delta\theta_t
\]</span></p>
<p><font color=red><span class="math inline">\(\gamma\)</span> 一般设定为 0.9</font></p>
<h3 id="缺点-5">缺点</h3>
<p>解决了 AdaGrad 的缺点</p>
<h3 id="代码-6">代码</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">grad_squared = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    dx = compute_gradient(x)</span><br><span class="line">    grad_squared = decay_rate * grad_squared + (<span class="number">1</span>-decay_rate) * dx * dx</span><br><span class="line">    x -= learning_rate * dx / (np.sqrt(grad_squared) + <span class="number">1e-7</span>)</span><br></pre></td></tr></table></figure>
<h3 id="适用范围-6">适用范围</h3>
<blockquote>
<p>RMSProp由Hinton于2012年提出，用于修改AdaGrad以在非凸设定下效果更好，将梯度积累改变为指数加权的移动平均。AdaGrad设计以让凸问题能够快速的收敛。当应用于非凸函数训练神经网络时，学习轨迹可能穿过了很多不同的结构，最终到达一个局部是凸碗的结构。AdaGrad根据平方梯度的整个历史来收缩学习率，学习率很可能在到达这样的凸碗结构之前就变得太小。而RMSProp使用指数衰减平均，丢弃遥远过去的历史，使其能够在找到凸碗结构后快速收敛，该算法等效于一个初始化与该碗状结构的AdaGrad算法。实践中和经验上，RMSProp已经被证明是是一种有效而且实用的深度神经网络优化算法，目前是深度学习从业者经常采用的优化方法之一</p>
</blockquote>
<h2 id="adam">Adam</h2>
<blockquote>
<p>Adam (adaptive moments)，在早期算法的背景下，最好被看成结合RMSProp和具有一些重要区别的动量的变种</p>
</blockquote>
<p><strong>相当于 RMSprop + Momentum</strong></p>
<h3 id="梯度更新规则-7">梯度更新规则</h3>
<p>除了像 Adadelta 和 RMSprop 一样存储了过去梯度的平方 vt 的指数衰减平均值 ，也像 momentum 一样保持了过去梯度 mt 的<strong>指数衰减平均值</strong> <span class="math display">\[
m_t=\beta_1m_{t-1}+(1-\beta_1)g_t
\]</span></p>
<p><span class="math display">\[
v_t=\beta_2v_{t-1}+(1-\beta_2){g_t}^2
\]</span></p>
<p>如果 mt 和 vt 被初始化为 0 向量，那它们就会向 0 偏置，所以做了<strong>偏差校正</strong>，通过计算偏差校正后的 mt 和 vt 来抵消这些偏差 <span class="math display">\[
\hat{m_t}=\frac {m_t} {1-{\beta_1}^t}
\]</span></p>
<p><span class="math display">\[
\hat{v_t}=\frac {v_t} {1-{\beta_2}^t}
\]</span></p>
<p><strong>规则</strong> <span class="math display">\[
\theta_{t+1}=\theta_t-\frac {\eta} {\sqrt{\hat{v_t}}+\epsilon}\hat{m_t}
\]</span> <font color=red><strong>建议 β1 ＝ 0.9，β2 ＝ 0.999，ϵ ＝ 10e−8</strong></font></p>
<h3 id="缺点-6">缺点</h3>
<p>并不适合所有</p>
<h3 id="代码-7">代码</h3>
<ul>
<li>beta1 = 0.9</li>
<li>beta2 = 0.999</li>
<li>learning_rate = 1e-3 or 5e-4</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">first_moment = <span class="number">0</span></span><br><span class="line">second_moment = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    dx = computed_gradient(x)</span><br><span class="line">    first_moment = beta1 * first_moment + (<span class="number">1</span>-beta1) * dx  <span class="comment"># Momentum</span></span><br><span class="line">    second_moment = beta2 * second_moment + (<span class="number">1</span>-beta2) * dx *dx <span class="comment">#AdaGrad/RMSProp</span></span><br><span class="line">    x -= learning_rate * first_moment / (np.sqrt(second_moment) + <span class="number">1e-7</span>)</span><br></pre></td></tr></table></figure>
<p>上面的Adam前几次迭代的步长会非常大，这里增加了偏置矫正项t： 注意t的值会随着迭代次数增加</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">first_moment = <span class="number">0</span></span><br><span class="line">second_moment = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(num_iterations):</span><br><span class="line">    dx = compute_gradient(x)</span><br><span class="line">    first_moment = beta1 * first_moment + (<span class="number">1</span>-beta1) * dx</span><br><span class="line">    second_moment = beta2 * second_moment = （<span class="number">1</span>-beta2) * dx * dx</span><br><span class="line">    first_unbias = first_moment / (<span class="number">1</span> - beta1 ** t) <span class="comment">#偏置纠正</span></span><br><span class="line">    second_unbias = second_moment / (<span class="number">1</span> - beta2 ** t)</span><br><span class="line">    x -= learning_rate * first_unbias / (np.sqrt(second_unbias) + <span class="number">1e-7</span>)</span><br></pre></td></tr></table></figure>
<h3 id="适用范围-7">适用范围</h3>
<blockquote>
<p>首先，在Adam中动量直接并入了梯度一阶矩（指数加权）的估计。将动量加入RMSProp最直观的方法是将动量应用于缩放后的梯度。其次，Adam包括偏置修正，修正从原点初始化的一阶矩（动量项）和二阶矩（非中心项）</p>
</blockquote>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="reward-container">
  <div>Buy me a coffee</div>
  <button>
    赞赏
  </button>
  <div class="post-reward">
      <div>
        <img src="https://cdn.jsdelivr.net/gh/whlll-coder/images/wechatpay.jpg" alt="whlll 微信">
        <span>微信</span>
      </div>
      <div>
        <img src="https://cdn.jsdelivr.net/gh/whlll-coder/images/alipay.jpg" alt="whlll 支付宝">
        <span>支付宝</span>
      </div>

  </div>
</div>

          <div class="post-tags">
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i> 深度学习</a>
              <a href="/tags/%E7%AE%97%E6%B3%95/" rel="tag"><i class="fa fa-tag"></i> 算法</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/10/31/Markdown%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E8%AF%AD%E6%B3%95/" rel="prev" title="Markdown数学公式语法">
                  <i class="fa fa-chevron-left"></i> Markdown数学公式语法
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2021/11/04/Hibernate/" rel="next" title="Hibernate">
                  Hibernate <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments" id="lv-container" data-id="city" data-uid="MTAyMC81NDU4My8zMTA1NA=="></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">whlll</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">373k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">5:39</span>
  </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@next-theme/pjax@0.5.0/pjax.min.js" integrity="sha256-3NkoLDrmHLTYj7csHIZSr0MHAFTXth7Ua/DDt4MRUAg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="/js/pjax.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>
<script class="next-config" data-name="chatra" type="application/json">{"enable":true,"async":true,"id":"cHCGeJJx2x2CH2DDt"}</script>
<script src="/js/third-party/chat/chatra.js"></script>
<script async src="https://call.chatra.io/chatra.js"></script>



  <script src="/js/third-party/fancybox.js"></script>


  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script src="/js/third-party/comments/livere.js"></script>

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","model":{"jsonPath":"/live2dw/assets/tororo.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"rect":"opacity:0.7","log":false,"tagMode":false});</script></body>
</html>
